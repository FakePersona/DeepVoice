\documentclass[a4paper,french,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{fullpage}
\usepackage[french]{babel}
\usepackage{graphicx}

\title{PROJ: DeepVoice}

\author{Rémi Hutin \and Rémy Sun \and Raphael Truffet}

\begin{document}

\maketitle


\begin{figure}[!h]
    \centering
    \includegraphics[scale=2]{squelette.jpg}
\end{figure}

\section{Introduction}

\section{Sound signals}

\subsection{Cepstral analysis}

\subsection{GMMs}

\subsection{Supervectors and i-vectors}

\subsection{Previous works}

\section{Use of neuronal networks}

\subsection{Formal neuron}

\paragraph{Neuron?}
A neuron can be thought of as a function which takes an $n$-dimensional vector $A$ as input and returns a scalar $e$ as output. This function typically has two internal parameters which are a bias $b$ and a weight-matrix $W$. The function starts by calculating $WA+b$ before using a non-linear activation function (such as sigmoid or tanh): $e=f(WA+b)$.

\paragraph{Adjusting the function}

[Explanation of the backpropagation with a very simple example, insist on the relation to a predetermined objective]

\paragraph{Neuronal network?}

[Explanation as to the organization in layers and the hierarchical distinctions learned through deep learning]

\subsection{Autoencoders}

\paragraph{A default goal}
[We bypass the need for labels by defaulting to an objective that does not require additional information on the input]

\paragraph{Structure}
[Basic canvas of what an autoencoder might look like]

\paragraph{Why use autoencoders?}
[Discussion on generalization processes and denoising autoencoders, explanation of our basic idea and tying it up with the projection issue we are tackling]

\subsection{Existing application}

\subsection{Tied weight autoencoder}

\paragraph{Architecture}
[Explanation]

\paragraph{Good results}
[Vedran's paper?]

\section{Method}




\end{document}
